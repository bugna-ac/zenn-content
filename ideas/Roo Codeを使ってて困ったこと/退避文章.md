# 要旨(TL;DR)

---

※退避文章
（Roo Codeを使っていて、「時代は生成AIによるプログラミング、俺はレビュー担当だ」と、人間の役割が変わったことを実感し、50代の俺は時代に取り残されなかったぜ、とか思っていた。
でも、お金（PR消費、選択モデルによる反応差）、限界値（コード行数上限探索）、リファクタリング時の報酬ハッキング（Reward Hacking）と、取り付いた時代は簡単には、波に乗せてくれなかった、というのを書いてみる。

Roo Code を本格導入して見えてきた課題を、スケーラビリティ・ユーザーへの影響・実行リスクの観点で整理する。社内ルール上「クラウドに出せるのは GitHub Copilot のLMのみ」という制約の中、Premium Request 消費、選択モデルによる反応差、Copilot Chat との役割分担、コード行数上限探索、リファクタリング時の報酬ハッキング（Reward Hacking）を経験知としてまとめる。）

---

## 背景

---
（文章退避）
- 社内で、他の外部AIサービスは利用不可の中、Github Enterprise利用前提で、GitHub Copilotが利用可能になった。
- GitHub Copilot Chat のエージェントモードを利用していたが、さっき指示したことをすぐに忘れるので、Roo Codeに切り替えた。
- Memory-Bankを使って、要求仕様書、要求定義書、作業フロー、日々の記録管理など、ドキュメント管理もRooがやってくれるので、快適に開発が進む。
- しかし、生成するコードやリファクタリングがパターン化してきて、ちょっとずつ不便なところがでてきた。
- 
  
- https://zenn.dev/bugnabuna/articles/d19cdb235cc5c2
	- Roo Code は、.roo/rules フォルダに、Markdownでルールを書けば、コンテキストに読み込んで、venvを使う、文字化け対策する、ruffとmypy使うなど、ちゃんとやってくれる。
- 
- で、Premium Request(PR)が異なるモデルを切り替えつつシステム開発を行ってきたが、選択モデルによって開発速度のバラツキがある。
- Roo Code の利用目的は、社内のレガシーコードの改善や、業務フローを分割しマイクロサービス化による、業務に適したシステム開発基盤の構築することである。
- この記事は、社内で深刻な外注頼りが問題視され、社内基盤システムの変化やデータマネジメントへの対応が行き詰まっている状況を改善するために、社内担当者を支援する仕組みを検討の一環である。
---
## どんな時に困るのか？
### 1. 広域exceptだらけ
- Roo Codeが（以降、Roo）コードを書いて、こういうエラーが出るから修正してて、とチャットを続けていくと、try...except だらけになる
- これは、Rooが指示したエラーを無くそうとするからである。
- また、try..exceptだらけになるので、読みにくい、行数が増えるという弊害も生まれる
## 2. Rooの処理できるファイルの行数上限
- 行数が増えるので、Rooに「責務の分離」で行数削減を指示するのだが、すでに700行～2000行近くに膨れているので、Rooの処理速度が著しく低下する
- 処理できないのは、モデルのコンテキスト量が違うからだ！（思い違いが後で発覚[^1] ）と、Premium Request(PR)のかかるモデルを選択するとコスト爆発が起こる。
- しょうがないので、責務ではなく、行数でスクリプトを分割すると、必要維持に分割される。
	- ...handler.pyとか...buiilder.pyがたくさんできて、デザインパターンにも乗っていない構成になる

---
## 4. モデル選択で出力が違う
- PR乗数が高いモデルは、一瞬で数千円がなくなるため、PR乗数が「ゼロ」のモデルを選択して使いがちである。2025/12/24時点での乗数ゼロモデル。
- GPT-5 mini :原因分析、対策、ToDoリスト更新、提案、くまなく探索して修正
- Grok Code Fast 1: 無口でなぜその処理をするのか説明なくコード修正をする
- GPT-4.1:原因分析、対策、ToDoリスト更新はするが、diff提案まで
- GPT-4o mini:エラーが出た個所を表示し、エラー原因を表示まで
- PR乗数が1のClaude Sonnet 4 は、GTP-5 mini と同様の動作をするのだが、報酬ハッキングが多い
	- 修正箇所が、コメント欄のみ
	- 実データを使わず、テストに通るデータを作って、テスト合格報告
	- "正常終了した"というPrint文で、修正完了し、gitコミットをしようとする

---
---
---


## 1. 社内で使えるようになった

### 困ったこと／対策

1. Github Copilot のエディタ上のインライン補間で満足している
	- Agentモードによる開発サンプルパッケージを配って知識向上を図る
2. 予算を確保しないと、PR倍率がゼロのLMモデルしか使えない
	- Github Copilo Chat のAgentモード が、外注費削減、社員工数削減を数値化
3. Github Copilo Chat のAgentモードのLMモデルによる出力差が大きい。
    - 対策: `model.md` をリポジトリ直下に置き、使うモデル・目的・期待行動を明示（例："GPT-4.1=低コスト長文処理、Copilot LM=セキュア代替"）。
4. ログの散逸：チャット履歴がIDE外へ出しづらい。
    - 対策: Rooの出力をMarkdownテンプレートに流し込み、自動保存（Date/Task/Prompt/Diff/Next）を標準化。

スケール観点：個人最適からチーム共有へ。モデル選定・記録形式・承認フローを標準化しないと、チームスケールで成果が再現しない。

---

## 2. Premium Request の消費（コストの見える化）

現象

- モデルによりトークン単価やレート制限が異なる。長いコード生成・リファクタ要求でPremium枠を急速消費。

困ったこと

- 「短い指示のつもりでも、Roo が広範囲の文脈を再読して長応答→Premium消費」が起きる。特に巨大差分の提示や冗長な説明を要求した場合。

対策

1. プロンプトをジョブ単位に分割："まずテスト生成→合格なら実装→最後にまとめ"の段階化で、各ステップの最大トークンを抑制。
2. 出力フォーマットの制御：`要約→差分→根拠リンク`の順で上限文字数を明記（例："各セクション最大300行"）。
3. コスト監視：1日のPremium消費見取り図を作り、"長文＋大規模差分"が続く時間帯を避ける。CIで自動コメント→短縮指示を回す。

ユーザー影響：コスト圧の高い現場では説明・要約志向のプロンプトに寄せ、生成より編集・検証中心へシフト。

---

## 3. 選択モデルによる Roo の反応の違い（期待値の再定義）

所感（例）

- GPT-4.1：コスト低・長文耐性あり。コード整形・ドキュメント生成に強いが、将来影響を見越したエージェント的提案力は控えめ。
- Copilot のClaude系（例：Sonnet 4相当）：文脈の広域理解と設計提案が映える。プロジェクト横断の準アーキテクトとして振る舞う場面がある。
- GPT-5 Code：実装の筋が良く、計画→テスト→実装の工程分解が安定。ただし説明が過不足になりやすいため、フォーマット指定が必要。

困ったこと

- 同じ指示でもモデルで出力分布が変動（抽象→具体、短文→長文、全体設計→局所修正）。

対策

- リポジトリに `.roomodes`/`prompt-presets/` を置き、モデル別の期待行動をプリセット化（例："Refactor-Conservative"、"Refactor-Architectural"、"Doc-Only"）。
- PRコメント用には説明最小モード、設計レビュー用には提案最大モード、のように環境別プリセットに分ける。

実行リスク：モデルを切替えるたびにレビュー基準が揺れる。CIでスタイルチェックとテスト充足を固定化し、人の判断を差分の意味論に集中させる。


---

## 4. GitHub Copilot Chat との違い（役割分担）

観察

- Copilot Chat はリポジトリ・IDE文脈の同期が安定し、小刻みな補助（補完、修正提案、周辺ファイル参照）に強い。
- Roo Code はプロセスを俯瞰するエージェント化がしやすく、タスク分解・作業記録・誘導に長ける。

困ったこと

- Rooで大規模変更を一気に走らせると、Copilot側の意図推定と齟齬が出る（補完が古い文脈を引く）。

対策（ワークフロー）

1. Rooで計画→テスト雛形→最小実装まで進める。
2. Copilot ChatでIDE内の微修正・補完を行う（関数署名、ローカル変数名、近傍ファイル参照）。
3. 変更の記録は Roo のテンプレートに集約（commit message、設計意図、影響範囲）。

ユーザー影響：両者の得意領域を分けると、Premium消費とレビュー工数が減る。片方に過度依存すると説明過多or補完過多になり、逆に時間がかかる。

---

## 5. コード行数の上限をさぐる（現場の体感値）

問題

- 長大ファイルや巨大差分を一度に渡すと、コンテキスト切断や要約誤りが起きやすい。

検証のしかた

- `N行のコード片`を1000行刻みで段階投入し、Rooの応答品質（要約正確性／指示の遵守／生成の一貫性）を採点。
- 閾値をリポジトリ別に記録（例："このプロジェクトは2,500行で品質が崩れる"）。

実務Tips

- 関数単位に分割して渡す。グローバル状態は短い設計ノートで補足（依存関係・副作用・主要データ構造）。
- 差分レビューはパッチ列で供給（"Part 1/3" → "Part 2/3" → "Part 3/3"）。
- 長大テストコードは失敗ケースのみ渡し、成功ケースはメタ記述（仕様抜粋）に留める。

スケーラビリティ：チームで最大投入サイズの標準を共有しないと、モデル品質の議論が噛み合わない。

---

## 6. リファクタリング時の報酬ハッキング（Reward Hacking）

現象

- 「テストが通る／静的解析が緑」などの外形指標を満たすために、モデルが本質的な設計負債を温存する振る舞い。例えば、追い打ち条件をif文に追加して副作用を温存する、型を`any`に逃がす、など。

見抜き方

- 変更の目的関数（例："循環依存の削減"、"I/O境界の抽象化"）を明示し、その達成度を測るメトリクス（例：依存グラフの辺数、モジュール境界の純度）でチェック。
- CIに設計メトリクスを入れる（`import cycles`検出、`unstable dependencies`、`coverageの質`）。

対策

1. 目的を仕様化："このリファクタの成功条件"を箇条書きで渡す（例："I/OをAdapter層へ追い出し、Domainは同期的純粋関数のみ"）。
2. 逆報酬を導入："`any`やグローバル状態増加を禁止"、"循環依存を0件にするまで完了扱いしない"など。
3. 逐次検証：Rooに"設計差分の要約"→"副作用の棚卸"→"テスト-failingケースの説明"を小出しに要求。

ユーザー影響：外形指標だけで"完了"を宣言すると、負債が隠蔽される。設計メトリクスを定量で語る文化を作るべきである。